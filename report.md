# LLM的微调和RAG：Retrieval-Augmented Generation (RAG) Meets Per-Domain Test-Time Training (TTT)

## Methodology

This project explores how the combination of Retrieval-Augmented Generation (RAG) and Test-Time Training (TTT) can enhance the performance of cutting-edge large language models (LLMs) on the challenging GPQA benchmark. The idea is simple: challenging benchmarks like GPQA require models to have both domain-specific knowledge and the ability of long-term reasoning; RAG provides the former, while TTT provides the latter. By combining these two techniques, we aim to improve the performance of LLMs on the GPQA benchmark.

### Core Concepts of RAG

Retrieval-Augmented Generation (RAG) enhances language model generation by integrating an external knowledge base (such as documents, web pages, or databases) to inform and augment the model’s outputs. This is particularly valuable in tasks like long-form question answering or domain-specific problem-solving (e.g., GPQA). In this project, we first chunk documents, and then use BM25 as the retriever and an LLM as the generator to implement RAG with modified `LightRAG` framework provided by TAs.

### Core Concepts of TTT

Test-Time Training (TTT) is a technique where a model is fine-tuned during the testing phase on data specific to the task or domain at hand. Unlike traditional training, TTT dynamically adapts the model to better handle the particular nuances of the test data. In this project, we perform per-domain TTT which means we collect domain-specific training data from the whole testset without using its groundtruth labels.

## Source of the Knowledge Base

The RAG knowledge base was constructed using the first 100,000 data chunks from the `OpenScholar/OpenScholar-DataStore-V3` dataset. These chunks provide a rich source of scholar domain knowledge to support the model’s generation process. Since the document is pre-chunked, we can directly use it to build the knowledge base. Importantly, **the knowledge base only provides complementary information to the model, and the model is still responsible for generating the final answer on its own internal knowledge**. It reflects actual human behavior where external knowledge is used to support reasoning and decision-making, and most real-world problems would not perfectly fit documented data.

## Implementation of RAG and Training on LLMs

The RAG approach in this project is built upon `meta-llama/Llama-3.1-8B-Instruct` and `THUDM/chatglm3-6b`. These models are hosted using `vllm` and are fine-tuned with `llama-factory` on H100 GPUs.

### RAG Configuration

The process of building the RAG knowledge base involves the following steps:

1. Use the first 100,000 chunks from the `OpenScholar/OpenScholar-DataStore-V3` dataset.
2. Tokenize and store the chunks in the database, which can be moved to external storage for retrieval.

The inference process for RAG involves retrieving the top-k ($k=5$) relevant chunks using BM25 and generating answers using the LLM. We prompt the LLM to cite the related document to the question, based on its inherent instruction-following capabilities. An example of QA process is as follows (generated by TTT-ed LLaMA-3.1-8B-Instruct):

```markdown
### Context
---Role---

You are a helpful assistant responding to questions about documents provided.

---Documents---

Document 1: Targeting microRNA/UHRF1 pathways as a novel strategy for cancer therapy Ubiquitin-like containing plant homeodomain and RING finger domains 1 (UHRF1) is an anti-apoptotic protein involved in the silencing of several tumor suppressor genes (TSGs) through epigenetic modifications including DNA methylation and histone post-translational alterations, and also epigenetic-independent mechanisms. UHRF1 overexpression is observed in a number of solid tumors and hematological malignancies, and is considered a primary mechanism in inhibiting apoptosis. UHRF1 exerts its inhibitory activity on TSGs by binding to functional domains and therefore influences several epigenetic actors including DNA methyltransferase, histone deacetylase 1, histone acetyltransferase Tat-interacting protein 60 and histone methyltransferases G9a and Suv39H1. UHRF1 is considered to control a large macromolecular protein complex termed epigenetic code replication machinery, in order to maintain epigenetic silencing of TSGs during cell division, thus enabling cancer cells to escape apoptosis. MicroRNAs (miRNAs) are able to regulate the expression of its target gene by functioning as either an oncogene or a tumor suppressor. In the present review, the role of tumor suppressive miRNAs in the regulation of UHRF1, and the importance of targeting the microRNA/UHRF1 pathways in order to induce the reactivation of silenced TSGs and subsequent apoptosis are discussed. The ECREM complex is considered to be orchestrated by UHRF1 to ensure a coordinated transmission of silenced TSGs to daughter cells during cell division (4,7,9). UHRF1 binds to H3K9me2, a repressive chromatin mark, thus providing additional evidence of UHRF1-mediated crosstalk between DNA methylation and histone modification (10). Furthermore, UHRF1 was also demonstrated to bind to H3K9me3 through the TTD
Document 2: of EZH2 or forced expression of methylatedpromoter-repressed BMP pathway restores normal differentiation capacity of CSCs. This reduces proliferation and induces terminal differentiation of CSCs, causing loss of self-renewal and a decrease in tumorigenicity of CSCs (31,32). Bmi1, a key component of the polycomb repressive complex 1 (PRC1), is upregulated in GBM and significantly enriched in the CSC population, but it is not expressed in normal astrocytes. Moreover, its suppression in human CSCs inhibits their growth in vitro and in vivo (33). Finally, the expression inhibition of Bmi1 by knockdown in a glioma mouse model suppresses the formation of malignant tumors (34). dna methylation DNA methylation, catalyzed by DNA methyltransferases (DNMTs), is a major epigenetic modification that modulates gene expression. DNA methylation patterns are heritable and reversible, conserved during cell division, and involved in cell reprogramming processes. DNA methylation directly represses gene expression via the inhibition of transcription factor recruitment (35). Transcription inhibition could also be indirect through the recruitment of methyl-CpG-binding proteins and their associated repressive chromatin remodeling activities (36). DNA methylation deregulation is found in cancer where hypermethylation of specific tumor suppressor genes (TSGs) leads to the inhibition of their expression. It is known that aberrant DNA methylation is linked to the initiation and the progression of cancer. Global DNA hypomethylation promotes chromosomal instability, reactivation of transposable elements, and loss of imprinting. Local hypomethylation induces oncogene activation, while hypermethylation induces the silencing of TSGs (37). Aberrant DNA methylation patterns have been detected in GBM. A well-studied example is the silencing of the DNA repair enzyme
Document 3: the woman could feel something rigid or a tumor in the breast. She may perform this examination when lying down. To examine the right side of her breast, she palpates it with her left hand and vice versa. She should palpate the breast in a circular movement to detect any tumor, mass or change in the skin. When infected with breast cancer, the breast shape changes and the nipples' shape and size change as well, the color of the breast changes, it becomes reddish... The woman should always take care of herself and look for any change in her breast. The woman should always when taking her shower and after the period, examine her breast looking for any problems and in the event she feels anything strange, she should immediately visit the physician. (Elias) You tell women: you stand in front of the mirror and you put your hand, because when she lifts her hand, she would feel her breast is tightened and it would be easier to feel any mass or change in the breast. Then, you should start to compress the breast, in the event there are any secretions with bad odors, this means that she has a problem in her breast. Third: lie down on the floor for example and compress the breast, if you feel any pain, this means that there is a problem in your breast. (Mukhtar) Although nine participants made no specific mention of BSE, the more typical comments were: The check-up can be done at home.
Document 4: cancer cells is coupled or uncoupled between these different omics layers. We previously reported that epigenetic silencing of RGS10 and RGS2 genes by HDAC1 (histone deacetylase 1) and DNMT1 (DNA methyltransferase 1) contributes to cisplatin chemoresistance in ovarian cancer [45,46]. The regulation of this and other genes known to have essential roles in resistance to therapy can be studied genome-wide using scNMT-seq at a single-cell resolution, incorporating data obtained in multiple epigenomic regulatory layers. Another possible use of this multiomics approach can be in identifying cancer cell subpopulations of which epigenetic silencing or activation of genes regulating chemoresistance (such as RGS10) is more profound relative to other cancer cell subpopulations in the same microenvironment for which epigenetic control does not contribute much to drug resistance. sci-CAR This method allows joint profiling of gene expression and chromatin accessibility in high-throughput since it uses split pool barcoding for thousands of single cells [47]. In this study, they applied sci-CAR to cells from a human lung carcinoma cell line collected at different time points after dexamethasone treatment. They were able to cluster untreated and treated cells based on transcriptome or chromatin accessibility data using unsupervised clustering or t-SNE visualization, showing the potential of this technique in scalable profiling of single-cell molecular phenotypes. Conclusion Most research on single-cell omics and multi-omics techniques have been consisted of proof-of-concept studies, and publications reporting the application of these methods in various aspects of cancer research are just beginning to emerge. With rapid advances in the field and reducing costs to perform such studies,
Document 5: the comments and revise your manuscript. Because the schedule for publication is very tight, it is a condition of publication that you submit the revised version of your manuscript before 04-Apr-2020. Please note that the revision deadline will expire at 00.00am on this date. If you do not think you will be able to meet this date please let me know immediately. To revise your manuscript, log into https://mc.manuscriptcentral.com/rsos and enter your Author Centre, where you will find your manuscript title listed under "Manuscripts with Decisions". Under "Actions," click on "Create a Revision." You will be unable to make your revisions on the originally submitted version of the manuscript. Instead, revise your manuscript and upload a new version through your Author Centre. When submitting your revised manuscript, you will be able to respond to the comments made by the referees and upload a file "Response to Referees" in "Section 6 -File Upload". You can use this to document any changes you make to the original manuscript. In order to expedite the processing of the revised manuscript, please be as specific as possible in your response to the referees. When uploading your revised files please make sure that you have: 1) A text file of the manuscript (tex, txt, rtf, docx or doc), references, tables (including captions) and figure captions. Do not upload a PDF as your "Main Document". 2) A separate electronic file of each figure (EPS or print-quality PDF preferred (either format should be produced directly from original creation package), or original software format) 3)

---------------

### Question

You are interested in studying a rare type of breast cancer in a mouse model. Your research up until now has shown that the cancer cells show low expression of a key tumor suppressor gene. You suspect that epigenetic mechanisms are at play. Which of these is the most suitable course of action to study the cause of gene silencing at your locus of interest?
Choices: 
(A) You perform RNA-sequencing in the cancer cells vs. healthy breast cells to measure global gene expression changes between the two cell populations.
(B) You perform CRISPR-mediated knockout of the DNMT3C gene in your cancer cell line in order to up-regulate DNA methyltransferase activity. You then test the expression of the tumor suppressor gene in the original cancer cells vs. the DNMT3C knock out.
(C) You carry out bisulphite sequencing at your locus of interest in your cancer cells and compare the patterns to healthy breast cells
(D) You use plasmid transfection to overexpress the Ras oncogene in your cancer cell line and compare the cellular phenotype to healthy cells.

Please think step by step and output the final answer in the format: [[X]] (X is A, B, C, or D), and denote any related document in the response in the format: ((X)) (X is the number of the document).

### Response

So I have this question about studying a rare type of breast cancer in a mouse model. My research so far shows that the cancer cells have low expression of a key tumor suppressor gene, and I suspect that epigenetic mechanisms are involved in silencing this gene. I need to choose the most suitable approach to study the cause of this gene silencing.

Let's look at the options:

(A) Perform RNA-sequencing in cancer cells vs. healthy breast cells to measure global gene expression changes.

(B) Perform CRISPR-mediated knockout of the DNMT3C gene in the cancer cell line to up-regulate DNA methyltransferase activity, then test the expression of the tumor suppressor gene.

(C) Carry out bisulfite sequencing at the locus of interest in cancer cells and compare patterns to healthy breast cells.

(D) Use plasmid transfection to overexpress the Ras oncogene in the cancer cell line and compare the cellular phenotype to healthy cells.

First, I need to understand what each of these approaches is trying to do.

Option (A) is about comparing global gene expression between cancer cells and healthy cells. While this could give me an idea of which genes are up-regulated or down-regulated in cancer, it's a broad approach and might not specifically address the epigenetic silencing of the tumor suppressor gene I'm interested in. It's more of a general expression profiling.

Option (B) involves knocking out DNMT3C, which is a DNA methyltransferase, in the cancer cell line. By doing this, I'm essentially reducing DNA methylation activity. Then, I test the expression of the tumor suppressor gene in the original cancer cells versus the DNMT3C knockout cells. This seems like it could help me understand if DNA methylation is involved in silencing the tumor suppressor gene. If knocking out DNMT3C leads to increased expression of the tumor suppressor gene, that would suggest that DNA methylation is repressing its expression.

Option (C) is bisulfite sequencing at the locus of interest in cancer cells compared to healthy cells. Bisulfite sequencing is a method used to detect DNA methylation patterns at single-base resolution. So, by doing this, I can directly see if there are differences in methylation patterns at the promoter or regulatory regions of the tumor suppressor gene between cancer and healthy cells. This seems very targeted and directly addresses the epigenetic aspect I'm interested in.

Option (D) is about overexpressing the Ras oncogene in the cancer cell line and comparing the cellular phenotype to healthy cells. While Ras is an oncogene, and overexpressing it might give me insights into cellular transformation, it doesn't directly address the epigenetic silencing of the tumor suppressor gene. It seems more relevant to studying the effects of oncogene activation rather than epigenetic regulation.

Given that my primary suspicion is that epigenetic mechanisms are causing the silencing of the tumor suppressor gene, I think options (B) and (C) are more directly relevant.

Option (B) is a functional assay where I'm manipulating DNA methylation levels and seeing the effect on gene expression. If knocking out DNMT3C leads to increased expression of the tumor suppressor gene, that would suggest that DNA methylation is repressing its expression.

Option (C) is a direct measurement of DNA methylation patterns at the locus of interest. If I see higher methylation levels in cancer cells compared to healthy cells, that could explain the gene silencing.

Between these two, option (C) seems more straightforward because it directly measures the epigenetic mark (DNA methylation) at the specific locus of interest, whereas option (B) is an indirect approach that manipulates DNA methylation and observes the effect on gene expression.

However, option (B) could also be useful because it can help confirm whether DNA methylation is indeed involved in silencing the gene. If knocking out DNMT3C leads to increased expression, that would support the idea that DNA methylation is repressing the gene.

But since the question is about studying the cause of gene silencing, and epigenetic mechanisms are suspected, I think option (C) is the most direct and suitable approach to investigate the epigenetic cause of the silencing.

Therefore, my answer is [[C]].

((3))

```

### Training Configuration

Model training in this project is performed using `llama-factory`, which facilitates distributed training on H100 GPUs. The specific training configurations are defined in YAML files, utilizing Supervised Fine-Tuning (SFT):

- `post_train/llama_inst_cft.yaml` represents TTT on LLaMA-3.1-8b-Instruct with majority voted data, requiring 8 GPUs.
- `post_train/llama_inst_rft.yaml`, `post_train/chatglm3_full_cft.yaml`, and `post_train/chatglm3_full_rft.yaml` represent TTT on LLaMA-3.1-8b-Instruct with groundtruth-based baseline data and ChatGLM3-6b-base with majority voted data and groundtruth-based baseline data, respectively, requiring 4 GPUs.

> **Why not using `ChatGLM-6b` but `LLaMA-3.1-8b-Instruct`?**  
> Because `ChatGLM-6b` only supports 8192 token length by nature, which is not enough for RAG and long-term reasoning.

## Collection of TTT Data

The per-domain TTT data is collected through majority voting with the `Qwen/QwQ-32B-Preview` model on the GPQA questions, aiming to distill long-term reasoning abilities in the specific domain without groundtruth labels. It is worth noting that the training data does not contain retrieved documents and citation marks.

> **Why not using retrieved documents in collecting TTT data?**  
> Because of two reasons.  
> First, the retrieved documents are not always correct, and the model may be misled by incorrect information, which could harm the data quality.  
> Second, as discussed above, the model should be able to generate answers based on its own knowledge, and the retrieved documents are only used to provide plug-and-play additional information, which should not be include in the training data.  
> However, the ability to cite and use the related document is still important, which is related to the inherent instruction-following capabilities of the model. We leave this auxiliary task to the future work, and did not include it in the training data.

Additionally, a ground truth-based baseline dataset is provided for comparison, which use the groundtruth labels to rejectively filter generated responses from `Qwen/QwQ-32B-Preview`.

## Hyper-Parameters of Per-Domain TTT

Each training sample is cutoff at 20480 tokens, and the hyper-parameters are configured as follows:

- Training data details:

    | Dataset | #Samples | #Correctly Answered Questions | #Covered Questions |
    |:-------|:--------:|:-----------------------------:|:------------------:|
    | *GPQA* |  -  |  -  |  *448*  |
    | Majority Voted Data | 3755 | 205 | 382 |
    | Rejectively Sampled Data | 2655 | 353 | 353  |

- Hyper-parameters of training process:
    - *CFT* (Collective Fine-Tuning) denotes the model is fine-tuned with majority voted training data
    - *RFT* (Rejective Fine-Tuning) denotes the model is fine-tuned with rejectively sampled training data

    | Training Recipe |  Batch Size  |  Learning Rate  |  Epochs  |  Validation Set Ratio  |
    |:---------------|:------------:|:---------------:|:--------:|:----------------------:|
    | ChatGLM3-6b-base + CFT |  16  |  2e-5  |  3  |  0.1  |
    | ChatGLM3-6b-base + RFT |  16  |  2e-5  |  3  |  0.1  |
    | LLaMA-3.1-8B-Instruct + CFT |  16  |  2e-6  |  2  |  0.1  |
    | LLaMA-3.1-8B-Instruct + RFT |  16  |  2e-6  |  2  |  0.1  |

## Training Records

Here are the plot of training loss for each training recipe. The pattern are distinct when fine-tuning pretrained LLMs and aligned LLMs:

- ChatGLM3-6b-base
    - CFT
    ![ChatGLM3-6b-base + CFT](./post_train/figs/chatglm3_cft_loss.png)
    - RFT
    ![ChatGLM3-6b-base + RFT](./post_train/figs/chatglm3_rft_loss.png)
- LLaMA-3.1-8B-Instruct
    - CFT
    ![LLaMA-3.1-8B-Instruct + CFT](./post_train/figs/llama3_1_inst_cft_loss.png)
    - RFT
    ![LLaMA-3.1-8B-Instruct + RFT](./post_train/figs/llama3_1_inst_rft_loss.png)

## Experimental Results

Experiments focused on evaluating both pretrained LLMs and those enhanced with RAG on the GPQA benchmark. The evaluation compared the performance of bare LLMs, LLMs fine-tuned with TTT, and LLMs further augmented with RAG.

### Evaluation Settings

GPQA benchmark contains 448 questions, and each question has 4 choices. The metric is accuracy, which is the proportion of correctly answered questions. The evaluation is conducted in following settings:

- **Bare LLMs:** 
    - **Pretrained Models** (e.g., ChatGLM3-6b-base) are evaluated with few-shot in-context learning.
    - **Aligned Models** (e.g., ChatGLM3-6b, LLaMA-3.1-8B-Instruct) are evaluated with zero-shot prompting.
- **TTT Models:** Following the evaluation of aligned models.
- **RAG:** With retrieved top-5 documents, the model is prompted to cite the related document to the question and generate the final answer in the zero-shot manner.

For ChatGLM3-6b, the maximum generated tokens are 4096, for LLaMA-3.1-8B-Instruct, the maximum generated tokens are 8192, and for TTT models, the maximum generated tokens are 20480.

### Results Analysis

| Category | Model | Accuracy |
|:--------|:-----|:-------:|
| Bare LLMs | ChatGLM3-6b-base (0-shot) | 23.2 |
| | ChatGLM3-6b-base (1-shot) | 25.4 |
| | ChatGLM3-6b-base (5-shot) | 26.3 |
| | ChatGLM3-6b | 21.4 |
| | LLaMA-3.1-8B-Instruct | 25.7 |
| TTT Models | ChatGLM3-6b-base + CFT | 26.8 |
| | ChatGLM3-6b-base + RFT | \* |
| | LLaMA-3.1-8B-Instruct + CFT | 25.4 |
| | LLaMA-3.1-8B-Instruct + RFT | 24.3 |
| RAG | ChatGLM3-6b + RAG | 18.1 |
| | ChatGLM3-6b + CFT + RAG | \* |
| | LLaMA-3.1-8B-Instruct + RAG | 29.0 |
| | LLaMA-3.1-8B-Instruct + CFT + RAG | 33.5 |

> \* denotes collapsed results, where the model failed to generate responses to most questions.

The following sections delve into the detailed analysis of the results presented in the table above.

### Performance of Bare LLMs

**ChatGLM3-6b-base:**
- **Zero-Shot (0-shot):** Achieved an accuracy of 23.2%, which serves as the baseline performance without any additional context or examples.
- **Few-Shot Learning:** 
  - **1-shot:** Improvement observed with an accuracy of 25.4%.
  - **5-shot:** Further enhancement to 26.3%, indicating that providing a few examples aids the model in better understanding and responding to the questions.
- **Aligned Version (ChatGLM3-6b):** Notably underperformed with an accuracy of 21.4%, which is lower than its base counterpart. This suggests that alignment processes may have inadvertently reduced the model's effectiveness on the GPQA benchmark.

**LLaMA-3.1-8B-Instruct:**
- **Zero-Shot Performance:** Recorded an accuracy of 25.7%, outperforming the zero-shot ChatGLM3-6b-base by approximately 2.5 percentage points. This indicates a stronger inherent capability in handling the GPQA questions without additional training.

### Impact of Test-Time Training (TTT)

**ChatGLM3-6b-base:**
- **Collective Fine-Tuning (CFT):** Achieved an accuracy of 26.8%, representing a modest improvement over the 5-shot baseline. This demonstrates that TTT can enhance the model's performance by adapting it to the specific domain of the GPQA benchmark.
- **Rejective Fine-Tuning (RFT):** The results collapsed, with the model failing to generate responses to most questions. This failure could be attributed to overfitting or instability introduced during the rejective fine-tuning process.

**LLaMA-3.1-8B-Instruct:**
- **CFT:** Showed a slight decrease in performance to 25.4%, which is marginally lower than its zero-shot counterpart. This suggests that collective fine-tuning may not significantly benefit this model or could introduce noise that slightly hampers its performance.
- **RFT:** Experienced a more substantial drop to 24.3%, indicating that rejective fine-tuning negatively impacts the model's ability to handle the GPQA questions effectively.

### Effectiveness of Retrieval-Augmented Generation (RAG)

**ChatGLM3-6b + RAG:**
- **Performance:** Recorded an accuracy of 18.1%, which is lower than the bare ChatGLM3-6b-base model. This decline suggests that the integration of RAG, in this configuration, may have introduced noise or irrelevant information that confused the model, leading to poorer performance.

**LLaMA-3.1-8B-Instruct + RAG:**
- **Performance:** Achieved an accuracy of 29.0%, marking a significant improvement over the bare LLaMA model. This indicates that RAG effectively supplements the model's internal knowledge with external information, enhancing its ability to answer GPQA questions accurately.

**Combined TTT and RAG:**
- **ChatGLM3-6b-base + CFT + RAG:** Collapsed, failing to generate responses for most questions. The combination of collective fine-tuning and RAG likely overwhelmed the model, resulting in instability.
- **LLaMA-3.1-8B-Instruct + CFT + RAG:** Excelled with an accuracy of 33.5%, representing the highest performance across all configurations. This demonstrates the synergistic effect of combining RAG with TTT, where the model benefits from both augmented retrieval capabilities and domain-specific fine-tuning.

### Summary of Key Observations

1. **RAG Augmentation Benefits:** The integration of RAG consistently improved the performance of the LLaMA-3.1-8B-Instruct model, especially when combined with TTT. This underscores the value of external knowledge sources in enhancing LLMs' reasoning and domain-specific capabilities.

2. **TTT Variability:** While TTT provided benefits in some configurations (e.g., ChatGLM3-6b-base + CFT), it also led to performance collapses in others, particularly when combined with RAG. This highlights the sensitivity of TTT to model architecture and the nature of fine-tuning data.

3. **Model-Specific Responses:** Different models responded uniquely to RAG and TTT. LLaMA-3.1-8B-Instruct showed robust improvements with RAG and combined TTT, whereas ChatGLM3-6b-base experienced instability when both techniques were applied together.

4. **Limitations of RFT:** Rejective Fine-Tuning (RFT) generally did not yield positive results and, in some cases, degraded performance. This suggests that the rejective sampling approach may not be optimal for the GPQA benchmark or the models used.

5. **Instruction-Following Capabilities:** The ability of the models to cite related documents, as facilitated by RAG, appears to contribute positively to their performance, especially in the LLaMA model.

## Conclusion

This study investigated the synergistic integration of Retrieval-Augmented Generation (RAG) and Test-Time Training (TTT) to enhance the performance of large language models (LLMs) on the GPQA benchmark. The experimental results reveal several key insights:

1. **RAG as a Valuable Augmentation:** Incorporating RAG significantly bolstered the LLaMA-3.1-8B-Instruct model's accuracy, demonstrating the effectiveness of external knowledge retrieval in supporting domain-specific question answering.

2. **TTT Enhances Domain Adaptation:** TTT contributed to improved performance in certain configurations, particularly when combined with RAG, highlighting its role in adapting models to specific domains through dynamic fine-tuning.

3. **Model-Specific Efficacy:** The benefits of RAG and TTT were not uniform across all models. While LLaMA-3.1-8B-Instruct thrived under combined augmentation, ChatGLM3-6b-base faced stability issues, indicating that model architecture and inherent capabilities influence the effectiveness of these techniques.

4. **Challenges with Fine-Tuning Approaches:** Rejective Fine-Tuning (RFT) did not yield favorable outcomes, suggesting the need for refined strategies in selecting and curating training data for TTT to avoid potential performance degradation.

5. **Synergistic Potential:** The highest accuracy was achieved by combining RAG with Collective Fine-Tuning (CFT) in the LLaMA-3.1-8B-Instruct model, underscoring the potential of leveraging multiple augmentation techniques to maximize LLM performance on complex benchmarks.

In conclusion, the fusion of RAG and TTT presents a promising avenue for enhancing LLM capabilities, particularly in specialized domains requiring both extensive knowledge retrieval and nuanced reasoning. Future work should explore optimizing fine-tuning strategies and expanding the applicability of these techniques to a broader range of models and benchmarks to further validate and extend these findings.
